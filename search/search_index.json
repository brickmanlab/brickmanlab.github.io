{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>Welcome to the Brickman Lab wiki!</p> <p>Here you can find documentation for our analysis workflows. For more information about our research, visit the Brickman Group website.</p>"},{"location":"#transcriptional-basis-for-cell-fate-choice","title":"Transcriptional basis for cell fate choice","text":"<p>The Brickman Group aims to understand the transcriptional basis for early embryonic lineage specification.</p> <p>We are interested in the dynamic mechanisms by which cells can both reversible prime towards a particular fate or undergo a transition into commitment.</p>"},{"location":"#publications","title":"Publications","text":"Selected publications <p>Wong, Y. F., Kumar, Y., Proks, M., Herrera, J. A. R., Rothov\u00e1,M. M., Monteiro, R. S., Pozzi, S., Jennings, R. E., Hanley, N. A., Bickmore, W. A., and Brickman, J. M. (2023). Expansion of ventral foregut is linked to changes in the enhancer landscape for organ-specific differentiation. Nature Cell Biology, doi: 10.1038/s41556-022-01075-8.</p> <p>Perera, M., Nissen, S. B., Proks, M., Pozzi, S., Monteiro, R. S., Trusina, A., and Brickman, J. M. (2022). Transcriptional heterogeneity and cell cycle regulation as central determinants of Primitive Endoderm priming. eLife, doi: 10.7554/eLife.78967.</p> <p>Rothov\u00e1, M. M., Nielsen, A. V., Proks, M., Wong, Y. F., Riveiro, A. R., Linneberg-Agerholm, M., David, E., Amit, I., Trusina, A., and Brickman, J. M. (2022). Identification of the central intermediate in the extra-embryonic to embryonic endoderm transition through single-cell transcriptomics. Nature Cell Biology, doi: 10.1038/s41556-022-00923-x.</p> <p>Riveiro, A. R., and Brickman, J. M. (2020). From pluripotency to totipotency: an experimentalist's guide to cellular potency. Development, doi: 10.1242/dev.189845.</p> <p>Hamilton, W.B., Mosesson, Y., Monteiro, R.S., Emdal, K.B., Knudsen, T.E., Francavilla, C., Barkai, N., Olsen, J.V. and Brickman, J.M. (2019). Dynamic lineage priming is driven via direct enhancer regulation by ERK. Nature, doi: 10.1038/s41586-019-1732-z.</p> <p>Weinert, B.T., Narita, T., Satpathy, S., Srinivasan, B., Hansen, B.K., Scholz, C., Hamilton, W.B., Zucconi, B.E., Wang, W.W., Liu, W.R., Brickman, J.M., Kesicki, E.A., Lai, A., Bromberg, K.D., Cole, P.A., and Choudhary, C. (2018). Time-Resolved Analysis Reveals Rapid Dynamics and Broad Scope of the CBP/p300 Acetylome. Cell 174, 231-244.e212, doi:10.1016/j.cell.2018.04.033.</p> <p>Anderson, K.G.V., Hamilton, W.B., Roske, F.V., Azad, A., Knudsen, T.E., Canham, M.A., Forrester, L.M., and Brickman, J.M. (2017). Insulin fine-tunes self-renewal pathways governing naive pluripotency and extra-embryonic endoderm. Nature Cell Biology 19, 1164-1177, doi:10.1038/ncb3617.</p> <p>Nissen, S.B., Perera, M., Gonzalez, J.M., Morgani, S.M., Jensen, M.H., Sneppen, K., Brickman, J.M., and Trusina, A. (2017). Four simple rules that are sufficient to generate the mammalian blastocyst. PLoS Biol 15, e2000737, doi:10.1371/journal.pbio.2000737.  *joint senior author</p> <p>Migueles, R.P., Shaw, L., Rodrigues, N.P., May, G., Henseleit, K., Anderson, K.G., Goker, H., Jones, C.M., de Bruijn, M.F., Brickman, J.M., and Enver, T. (2017). Transcriptional regulation of Hhex in hematopoiesis and hematopoietic stem cell ontogeny. Developmental Biology 424, 236-245, doi:10.1016/j.ydbio.2016.12.021.</p> <p>Illingworth, R.S., H\u00f6lzenspies, J.J., Roske, F.V., Bickmore, W.A., and Brickman, J.M. (2016). Polycomb enables primitive endoderm lineage priming in embryonic stem cells. Elife 5, doi:10.7554/eLife.14926.</p> <p>Martin Gonzalez, J., Morgani, S.M., Bone, R.A., Bonderup, K., Abelchian, S., Brakebusch, C., and Brickman, J.M. (2016). Embryonic Stem Cell Culture Conditions Support Distinct States Associated with Different Developmental Stages and Potency. Stem Cell Reports 7, 177-191, doi:10.1016/j.stemcr.2016.07.009.</p>"},{"location":"#datasets","title":"Datasets","text":"<p>Rothova et al., (2022). Nature Cell Biology. Single-cell RNA-seq datasets from FOXA2Venus reporter mouse embryos and embryonic stem cell differentiation towards endoderm.</p>"},{"location":"rdm-guidelines/","title":"Research Data Management Guidelines","text":"<p>This section provides guidelines for effective research data management within our lab. By adopting these guidelines, we aim to improve data organization and naming conventions, leading to enhanced data governance and research efficiency. The guidelines include the following steps:</p> <ol> <li>Adhere to folder structure and naming conventions for <code>Assays</code> and <code>Projects</code> folders.</li> <li>Add relevant metadata to a <code>metadata.yml</code> in each folder</li> <li>Create a database from metadata files in <code>Assays</code> and <code>Projects</code> folders.</li> <li>Visualize database with a Panel python app.</li> <li><code>Projects</code> folders will be version controlled with Github and the Brickman organization.</li> <li><code>Projects</code> reports will be displayed under the Brickman organization GitHub Pages.</li> <li><code>Projects</code> will be syncronized and archived in Zenodo, which will give a DOI that can be used in a publication.</li> <li>NGS <code>Assays</code> folder will be uploaded to GEO, with the information provided in the metadata file.</li> <li>Create a Data Management Plan template that it is prefilled with repetitive information using DMPonline</li> </ol>"},{"location":"rdm-guidelines/#1-folder-structure-and-organization","title":"1. Folder structure and organization","text":"<p>To ensure efficient data management, it is important to establish a consistent approach to organizing research data. We consider the following practices:</p> <ul> <li>Folder structure: we aim to a logical and intuitive folder structure that reflects the organization of research projects and experimental data. We use descriptive folder names to make it easy to locate and access specific data files.</li> <li>Subfolders: Use subfolders to further categorize data based on their contents, such as code notebooks, results, reports, etc. This helps to keep data organized and facilitates quick retrieval.</li> <li>File naming conventions: implement a standardized file naming convention to ensure consistency and clarity. Use descriptive names that include relevant information, such as type of plots, results tables, etc.</li> </ul>"},{"location":"rdm-guidelines/#11-template-engine","title":"1.1 Template engine","text":"<p>We are currently using a cookiecutter template to generate a folder structure. Use cruft when generating assay and project folders to allow us to validate and sync old templates with the latest version.</p>"},{"location":"rdm-guidelines/#12-assay-folder","title":"1.2 Assay folder","text":"<p>For each NGS experiment there should be an <code>Assay</code> folder that will contain all experimental datasets (raw files and pipeline processed files). Inside <code>Assay</code> there will be subfolders named after a unique NGS ID and the date it was created:</p> <pre><code>&lt;Assay-ID&gt;_YYYYMMDD\n</code></pre>"},{"location":"rdm-guidelines/#assay-id-code-names","title":"Assay ID code names","text":"<ul> <li><code>CHIP</code>: ChIP-seq</li> <li><code>RNA</code>: RNA-seq</li> <li><code>ATAC</code>: ATAC-seq</li> <li><code>SCR</code>: scRNA-seq</li> <li><code>PROT</code>: Mass Spectrometry Assay</li> <li><code>CAT</code>: Cut&amp;Tag</li> <li><code>CAR</code>: Cut&amp;Run</li> <li><code>RIME</code>: Rapid Immunoprecipitation Mass spectrometry of Endogenous proteins</li> </ul> <p>For example <code>CHIP_20230101</code> is a ChIPseq assay made on 1st January 2023.</p>"},{"location":"rdm-guidelines/#folder-structure","title":"Folder Structure","text":"<pre><code>CHIP_20230424\n\u251c\u2500\u2500 description.yaml\n\u251c\u2500\u2500 metadata.yaml\n\u251c\u2500\u2500 pipeline.md\n\u251c\u2500\u2500 processed\n\u2514\u2500\u2500 raw\n   \u251c\u2500\u2500 .fastq.gz\n   \u2514\u2500\u2500 samplesheet.csv\n</code></pre> <ul> <li>description.yaml: short and long descriptions of the assay in yaml format.</li> <li>metadata.yaml: metadata file for the assay describing different keys (see below).</li> <li>pipeline.md: description of the pipeline used to process raw data.</li> <li>processed: folder with results of the preprocessing pipeline. Contents depend on the pipeline used.</li> <li>raw: folder with the raw data.</li> <li>.fastq.gz:In the case of NGS assays, there should be fastq files.</li> <li>samplesheet.csv: file that contains metadata information for the samples. This file is used to run the nf-core pipelines. Ideally, it will also contain a column with info regarding the experimental variables and batches so it can be used for down stream analysis as well.</li> </ul>"},{"location":"rdm-guidelines/#13-project-folder","title":"1.3 Project folder","text":"<p>There should be another folder called <code>Projects</code> that will contain project information and data analysis.</p> <p>A project may use one or more assays to answer a scientific question. This should be, for example, all the data analysis related to a publication.</p> <p>The project folder should be named after a unique identifier, such as:</p> <pre><code>&lt;Project-ID&gt;_YYYYMMDD\n</code></pre> <p><code>&lt;Project-ID&gt;</code> should be the surname of the owner of the project folder and the publication year, e.g. <code>JARH_etal_20230101</code>.</p>"},{"location":"rdm-guidelines/#folder-structure_1","title":"Folder structure","text":"<pre><code>&lt;Project-ID&gt;_20230424\n\u251c\u2500\u2500 data\n\u2502  \u251c\u2500\u2500 assays\n\u2502  \u251c\u2500\u2500 external\n\u2502  \u2514\u2500\u2500 processed\n\u251c\u2500\u2500 documents\n\u251c\u2500\u2500 notebooks\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 reports\n\u2502  \u2514\u2500\u2500 figures\n\u251c\u2500\u2500 requirements.txt\n\u251c\u2500\u2500 results\n\u2514\u2500\u2500 scripts\n</code></pre> <ul> <li>data: folder that contains symlinks or shortcuts to where the data is, avoiding copying and modification of original files.</li> <li>documents: folder containing word documents, slides or pdfs related to the project, such as explanations of the data or project, papers, etc.</li> <li>notebooks: folder containing Jupyter, R markdown or Quarto notebooks with the actual data analysis. Using annotated notebooks is ideal for reproducibility and readability purposes. Notebooks should be labeled numerically in order they were created e.g. <code>00_preprocessing</code></li> <li>README.md: detailed description of the project in markdown format.</li> <li>reports: notebooks rendered as html/docx/pdf versions, ideal for sharing with colleagues and also as a formal report of the data analysis procedure.</li> <li>figures: figures produced upon rendering notebooks. The figures will be saved under a subfolder named after the notebook that created them. This is for provenance purposes so we know which notebook created which figures.</li> <li>results: results from the data analysis, such as tables with differentially expressed genes, enrichment results, etc. These results should be saved under a subfolder named after the notebook that created them. This is for provenance purposes so we know which notebook created which results.</li> <li>scripts: folder containing helper scripts needed to run data analysis or reproduce the work of the folder</li> </ul>"},{"location":"rdm-guidelines/#14-synchronization-with-dangpu-server","title":"1.4 Synchronization with DanGPU server","text":"<p>We will have to setup a cron job to perform one-way sync between the <code>/projects</code> folder and <code>NGS_data</code> folder. All the analysis will be done on danGPU server, with no exceptions!</p> <p>After project is done and published, it will be moved to <code>NGS_data</code>.</p>"},{"location":"rdm-guidelines/#15-general-naming-conventions-and-more-info","title":"1.5 General naming conventions and more info","text":"<ul> <li>date format: <code>YYYYMMDD</code></li> </ul> <p>Transcriptomics metadata standards and fields</p> <p>More info on naming conventions for different types of files and analysis is in development.</p>"},{"location":"rdm-guidelines/#2-metadata-and-documentation","title":"2. Metadata and documentation","text":"<p>Accurate documentation and metadata play a crucial role in facilitating data discovery and interpretation. Consider the following guidelines:</p> <ul> <li>Metadata capture: Record essential metadata for each dataset, including type of experiment, date, organisms, etc. This information provides context and helps others understand and reuse the data effectively.</li> <li>Readme files: Create readme files for each project or dataset. These files should provide a brief overview of the project, list the files and their descriptions, and explain any specific instructions or dependencies required for data analysis.</li> </ul>"},{"location":"rdm-guidelines/#21-assay-metadata-fields","title":"2.1 Assay metadata fields","text":"Metadata field Definition Format ontology example assay_ID Identifier for the assay that is at least unique within the project &lt;code_name&gt;_YYYYMMDD nan CHIP_20200101 assay_type The type of experiment performed, eg ATAC-seq or seqFISH nan ontology field- e.g. EFO or OBI ChIPseq assay_subtype More specific type or assay like bulk nascent RNAseq or single cell ATACseq nan ontology field- e.g. EFO or OBI bulk ChIPseq owner Owner of the assay (who made the experiment?). &lt;First Name&gt; &lt;Last Name&gt; nan Jose Romero platform The type of instrument used to perform the assay, eg Illumina HiSeq 4000 or Fluidigm C1 microfluidics platform nan ontology field- e.g. EFO or OBI Illumina extraction_method Technique used to extract the nucleic acid from the cell nan ontology field- e.g. EFO or OBI nan library_method Technique used to amplify a cDNA library nan ontology field- e.g. EFO or OBI nan external accessions Accession numbers from external resources to which assay or protocol information was submitted nan eg protocols.io, AE, GEO accession number, etc GSEXXXXX codename Internal code name for easy identification (like a keyword) nan nan Oct4_ChIP date Date of assay creation YYYYMMDD nan 20200101 nsamples Number of samples analyzed in this assay &lt;integer&gt; nan 9 is_paired Paired fastq files or not &lt;single OR paired&gt; nan single pipeline Pipeline used to process data nan nan nf-core/chipseq strandedness The strandedness of the cDNA library &lt;+ OR - OR *&gt; nan * processed_by Who processed the data &lt;First Name&gt; &lt;Last Name&gt; nan Sarah Lundregan organism Organism origin &lt;Genus species&gt; nan Mus musculus origin If it is internal or external data &lt;internal OR external&gt; nan nan path Path to files &lt;/path/to/file&gt; nan nan short_desc Short description of the assay plain text nan Oct4 ChIP after pERK activation"},{"location":"rdm-guidelines/#22-project-metadata-fields","title":"2.2 Project metadata fields","text":"<p>In development.</p>"},{"location":"rdm-guidelines/#3-data-management-catalogue","title":"3. Data management catalogue","text":"<p>@SLundregan is in the process of building a prototype for <code>Assay</code>, using the metadata contained in all <code>description.yml</code> and <code>metadata.yml</code> files in the assay folder. This will be in the form of an SQLite database that that is easily updatable by running a helper script.</p>"},{"location":"rdm-guidelines/#4-database-browser","title":"4. Database browser","text":"<p>@SLundregan is also working on a browsable database using Panel python app. The app will display the latest version of the SQLite database. Clicking on an item from the database will open a tab containing all available metadata for the assay.</p> <p>Also, it would be nice if you can create an <code>Assay</code> folder directly from there, making it easy to fill up the info for the metadata and GEO submission (see below)</p> <p>In the future, you could ideally visualize an analysed single cell RNAseq dataset by opening Cirrocumulus session.</p>"},{"location":"rdm-guidelines/#5-projects-version-control","title":"5. <code>Projects</code> version control","text":"<p>All projects should be version controlled using GitHub under the Brickman organization. After creating a cookiecutter template, initiate a git repository on the folder. The Git repository can stay private until it is ready for publication.</p>"},{"location":"rdm-guidelines/#6-projects-github-pages","title":"6. <code>Projects</code> GitHub pages","text":"<p>Using GitHub pages, it is possible to display your data analyses (or anything related to the project) inside the <code>Projects</code> folder so that they are open to the public in a html format. This is great for transparency and reproducibility purposes. This can be done after the paper has been made public (it is not possible to do with a private repository without paying).</p> <p>Info on how this is done should be put here</p>"},{"location":"rdm-guidelines/#7-project-archiving-in-zenodo","title":"7. <code>Project</code> archiving in Zenodo","text":"<p>Before submitting, link the repository to Zenodo and then create a Git release. This release will be caught by Zenodo and will give you a DOI that you can submit along the manuscript.</p>"},{"location":"rdm-guidelines/#8-data-upload-to-geo","title":"8. Data upload to GEO","text":"<p>The raw data from NGS experiments will be uploaded to the Gene Expression Omnibus (GEO). Whenever a new Assay folder is created, the data owner must fill up the required documentation and information needed to make the GEO submission as smooth as possible.</p>"},{"location":"rdm-guidelines/#9-create-a-data-management-plan","title":"9. Create a Data Management Plan","text":"<p>We are currently working on a DMP template that it is prefilled with repetitive information using DMPonline and the KU guidelines. This template will contain all the necessary information regarding common practices that we will use, the repositories we use for NGS, etc etc.</p> <p>more info on what is a DMP and how to use it</p>"},{"location":"starting-assay-project/","title":"Starting a new assay or project","text":"<p>Whenever you obtain sequencing data from Genomic's Platform, you have to create an Assay. By running the commands below, you will have option to fill all required information about the experiment. This workflow will help us with tracking of all sequencing done in our lab.</p>"},{"location":"starting-assay-project/#assay","title":"Assay","text":"<pre><code>module load miniconda/latest\nsource activate brickman\n\ncd /home/$USER/Brickman/assays\ncruft create https://github.com/brickmanlab/ngs-template --directory=\"assay\"\n</code></pre> <p>Next, please copy all <code>fastq</code> files from CPR share to folder <code>data/raw/fastq</code>. All processed data from a pipeline should be copied to <code>data/processed</code> folder.</p> <pre><code>rsync -avzh --progress --chmod=2775 ~/ucph/ndir/SUN-CPR-genomics_data/... &lt;ASSAY_ID&gt;/data/raw/fastq\n</code></pre> <p>After copying all the files, please run the last command.</p> <pre><code>chmod -R 775 /maps/projects/dan1/data/Brickman/assays/&lt;ASSAY_ID&gt;\n</code></pre>"},{"location":"starting-assay-project/#project","title":"Project","text":"<p>Every time you want to make some analysis, you should create a project. Our folder structure will allow you to easily link various experiments to your project and make your analysis easier.</p> <p>Please use the following naming convention: <code>surname-&lt;YOUR_CODENAME&gt;</code></p> <pre><code>module load miniconda/latest\nsource activate brickman\n\ncd /home/$USER/Brickman/projects\ncruft create https://github.com/brickmanlab/ngs-template --directory=\"project\"\n</code></pre> <p>Link required assays to your project.</p> <pre><code>ln -s /maps/projects/dan1/data/Brickman/assays/&lt;ASSAY_ID&gt; /maps/projects/dan1/data/Brickman/projects/&lt;PROJECT_ID&gt;/data/assays/\n</code></pre> <p>Link external data if needed</p> <pre><code>ln -s /maps/projects/dan1/data/Brickman/shared /maps/projects/dan1/data/Brickman/projects/&lt;PROJECT_ID&gt;/data/external/\n</code></pre>"},{"location":"tools/","title":"Bioinformatics tools","text":"Tool Description NGS Language Link Functional enrichment on genomic regions CHIP-seq ATAC-seq R https://github.com/jokergoo/rGREAT Pseudotime inference scRNA-seq Python https://github.com/LouisFaure/scFates nan Single-cell analysis package scRNA-seq Python https://github.com/scverse/scanpy nan AI probabilistic package for transfer learning DR and more scRNA-seq Python https://github.com/scverse/scvi-tools Gene set enrichment analysis on steroids scRNA-seq Python https://github.com/zqfang/GSEApy nan UpsetR on stereoids (complicated Venn Diagrams) Plotting R https://github.com/krassowski/complex-upset nan Complex heatmap Plotting Python https://github.com/DingWB/PyComplexHeatmap nan"},{"location":"dangpu/","title":"DanGPU","text":"<p>For starting on the server make sure to read:</p> <ul> <li>DanGPU manual</li> <li>Genomics Platform wiki</li> <li>platforms: JupyterHub and RStudio</li> </ul>"},{"location":"dangpu/#first-time-on-server","title":"First time on server","text":"<p>We wrote a small script which will setup everything for you. Login to the server and run the following command.</p> <pre><code>sh /maps/projects/dan1/data/Brickman/helper-scripts/helper-scripts/brickman-setup.sh\n</code></pre>"},{"location":"dangpu/#working-with-modules","title":"Working with modules","text":"<pre><code>module load miniconda/latest\n</code></pre>"},{"location":"dangpu/#conda","title":"Conda","text":"<p>If you work with <code>conda</code> you can use <code>mamba</code> instead, which is faster tool to install packages.</p> <p>We created shared <code>conda</code> environments to simplify your life.</p> <ul> <li>To list all available envs: <code>conda env list</code></li> <li>To activate env: <code>source activate brickman</code></li> </ul>"},{"location":"dangpu/#creating-own-shared-environment","title":"Creating own shared environment","text":"<p>Here is an example how we created shared environment called <code>brickman</code>.</p> <pre><code>module load miniconda/latest\n\nconda create --prefix /maps/projects/dan1/data/Brickman/conda/envs/brickman python=3.10\nsource activate brickman\npip install cruft cookiecutter\n\nchmod -R 755 /maps/projects/dan1/data/Brickman/conda/envs/brickman\n</code></pre>"},{"location":"dangpu/alphafold2/","title":"Alphafold 2","text":""},{"location":"dangpu/alphafold2/#1-running","title":"1. Running","text":""},{"location":"dangpu/alphafold2/#11-create-a-target-file","title":"1.1 Create a target file","text":"<pre><code># cat target.fasta\n&gt;query\nMAAHKGAEHHHKAAEHHEQAAKHHHAAAEHHEKGEHEQAAHHADTAYAHHKHAEEHAAQAAKHDAEHHAPKPH\n</code></pre>"},{"location":"dangpu/alphafold2/#12-setup-environments","title":"1.2. Setup environments","text":"<pre><code>srun -N 1 --ntasks-per-node=10 --gres=gpu:2 --pty bash\nmodule load miniconda/latest cuda/11.4 cudnn/8.2.2\nsource activate /maps/projects/dan1/data/Brickman/conda/envs/af2\n\ncd /maps/projects/dan1/data/Brickman/alphafold\nexport AF2_DATA_DIR=\"~/projects/data/Alphafold2/24022023\"\n</code></pre>"},{"location":"dangpu/alphafold2/#13-run-monomer-cli","title":"1.3. Run monomer (cli)","text":"<pre><code>python run_alphafold.py \\\n--fasta_paths=~/projects/data/Brickman/target_01.fasta \\\n--output_dir=/scratch/tmp/alphatest \\\n--model_preset=monomer \\\n--db_preset=full_dbs \\\n--data_dir=$AF2_DATA_DIR \\\n--uniref30_database_path=$AF2_DATA_DIR/uniref30/UniRef30_2021_03 \\\n--uniref90_database_path=$AF2_DATA_DIR/uniref90/uniref90.fasta \\\n--mgnify_database_path=$AF2_DATA_DIR/mgnify/mgy_clusters_2022_05.fa \\\n--pdb70_database_path=$AF2_DATA_DIR/pdb70/pdb70 \\\n--template_mmcif_dir=$AF2_DATA_DIR/pdb_mmcif/mmcif_files/ \\\n--obsolete_pdbs_path=$AF2_DATA_DIR/pdb_mmcif/obsolete.dat \\\n--bfd_database_path=$AF2_DATA_DIR/bfd/bfd_metaclust_clu_complete_id30_c90_final_seq.sorted_opt \\\n--max_template_date=2022-01-01 \\\n--use_gpu_relax\n</code></pre>"},{"location":"dangpu/alphafold2/#14-run-multimer-cli","title":"1.4. Run multimer (cli)","text":"<p>The example below generates 10 models.</p> <pre><code>python run_alphafold.py \\\n--fasta_paths=/home/fdb589/projects/data/Brickman/WTPU_1_WTC_EBPa.fasta \\\n--output_dir=/scratch/tmp/alphatest \\\n--model_preset=multimer \\\n--db_preset=full_dbs \\\n--data_dir=$AF2_DATA_DIR \\\n--uniref30_database_path=$AF2_DATA_DIR/uniref30/UniRef30_2021_03 \\\n--uniref90_database_path=$AF2_DATA_DIR/uniref90/uniref90.fasta \\\n--mgnify_database_path=$AF2_DATA_DIR/mgnify/mgy_clusters_2022_05.fa \\\n--template_mmcif_dir=$AF2_DATA_DIR/pdb_mmcif/mmcif_files/ \\\n--obsolete_pdbs_path=$AF2_DATA_DIR/pdb_mmcif/obsolete.dat \\\n--pdb_seqres_database_path=$AF2_DATA_DIR/pdb_seqres/pdb_seqres.txt \\\n--uniprot_database_path=$AF2_DATA_DIR/uniprot/uniprot.fasta \\\n--bfd_database_path=$AF2_DATA_DIR/bfd/bfd_metaclust_clu_complete_id30_c90_final_seq.sorted_opt \\\n--max_template_date=2022-01-01 \\\n--num_multimer_predictions_per_model=10 \\\n--use_gpu_relax\n</code></pre>"},{"location":"dangpu/alphafold2/#15-example-sbatch-script","title":"1.5. Example SBATCH script","text":"<pre><code>#!/bin/bash\n#SBATCH --job-name=AF2\n#SBATCH --gres=gpu:2\n#SBATCH --cpus-per-task=10\n#SBATCH --mail-type=BEGIN,END\n#SBATCH --mail-user=YOUR-EMAIL\nmodule load miniconda/latest cuda/11.4 cudnn/8.2.2\nsource activate /maps/projects/dan1/data/Brickman/conda/envs/af2\ncd ~/projects/data/Brickman/alphafold\nmkdir -p /scratch/tmp/alphatest\nexport AF2_DATA_DIR=\"~/projects/data/Alphafold2/24022023\"\nsrun python run_alphafold.py \\\n--fasta_paths=~/projects/data/Brickman/target_01.fasta \\\n--output_dir=/scratch/tmp/alphatest \\\n--model_preset=monomer \\\n--db_preset=full_dbs \\\n--data_dir=$AF2_DATA_DIR \\\n--uniref30_database_path=$AF2_DATA_DIR/uniref30/UniRef30_2021_03 \\\n--uniref90_database_path=$AF2_DATA_DIR/uniref90/uniref90.fasta \\\n--mgnify_database_path=$AF2_DATA_DIR/mgnify/mgy_clusters_2022_05.fa \\\n--pdb70_database_path=$AF2_DATA_DIR/pdb70/pdb70 \\\n--template_mmcif_dir=$AF2_DATA_DIR/pdb_mmcif/mmcif_files/ \\\n--obsolete_pdbs_path=$AF2_DATA_DIR/pdb_mmcif/obsolete.dat \\\n--bfd_database_path=$AF2_DATA_DIR/bfd/bfd_metaclust_clu_complete_id30_c90_final_seq.sorted_opt \\\n--max_template_date=2022-01-01 \\\n--use_gpu_relax\n</code></pre>"},{"location":"dangpu/alphafold2/#2-installation","title":"2. Installation","text":"<pre><code>conda create --prefix /maps/projects/dan1/data/Brickman/conda/envs/af2 python=3.8\nsource activate /maps/projects/dan1/data/Brickman/conda/envs/af2\n\nmamba install hmmer\npip install py3dmol\nmamba install pdbfixer==1.7\nmamba install -c conda-forge openmm=7.5.1\n\ncd /maps/projects/dan1/data/Brickman/\ngit clone --branch main https://github.com/deepmind/alphafold alphafold\npip install -r ./alphafold/requirements.txt\npip install --no-dependencies ./alphafold\n\n# stereo chemical props needs to be in common folder\nwget \u2013q \u2013P  /maps/projects/dan1/data/Brickman/alphafold/alphafold/common/ https://git.scicore.unibas.ch/schwede/openstructure/-/raw/7102c63615b64735c4941278d92b554ec94415f8/modules/mol/alg/src/stereo_chemical_props.txt\n\n# skipping content part\nmkdir -p ./alphafold/data/params &amp;&amp; cd ./alphafold/data/params\nwget https://storage.googleapis.com/alphafold/alphafold_params_colab_2022-12-06.tar\ntar --extract --verbose --preserve-permissions --file alphafold_params_colab_2022-12-06.tar\npip install ipykernel ipywidgets tqdm\npip install --upgrade scprep phate\n\n# Install jax\nmodule load miniconda/latest\nmodule load cuda/11.4 cudnn/8.2.2\nexport CUDA_VISIBLE_DEVICES='3'\npip install \"jax[cuda11_cudnn82]\" -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\n\n# fix last issues\nmamba install -c conda-forge -c bioconda hhsuite\nmamba install -c bioconda kalign3\npip install numpy==1.21.6\n</code></pre>"},{"location":"dangpu/alphafold2/#21-download-references","title":"2.1. Download references","text":"<p>Note</p> <p>Downloading references will not work on one try, had to do a lot of manual re-running of scripts.</p> <pre><code># create folder\nmkdir -p ~/projects/data/Alphafold2/24022023\ncd ~/projects/data/Alphafold2/24022023\n\n# Download all databases\nsh download_all_data.sh ~/projects/data/Alphafold2/24022023/ &gt; download.log 2&gt; download_all.log\n\n# Some fix-ups\n# mmCIF will not work because the firewall blocks the port, so I found this workaroud online\n# ref: https://github.com/deepmind/alphafold/issues/196\nwget -e robots=off -r --no-parent -nH --cut-dirs=7 -q ftp://ftp.ebi.ac.uk/pub/databases/pdb/data/structures/divided/mmCIF/ -P \"${RAW_DIR}\"\n# Last step is to fix all the permissions\nchmod -R 755 24022023/\n</code></pre>"},{"location":"dangpu/alphafold2/#references","title":"References","text":"<ul> <li>ifb-elixirfr</li> <li>deepmind/alphafold</li> </ul>"},{"location":"dangpu/ku-computer/","title":"KU computer setup","text":""},{"location":"dangpu/ku-computer/#conda","title":"Conda","text":"<p>Go here and download Miniconda PKG not BASH. If you're running M1/2 please follow this guideline.</p>"},{"location":"dangpu/ku-computer/#example-for-chip-seq-setup","title":"Example for CHIP-seq setup","text":"<pre><code>conda create --name chipseq python=3.6\nconda activate chipseq\nconda install -c bioconda deeptools bedtools\npip install intervene\n</code></pre>"},{"location":"dangpu/nextflow_tower/","title":"Nextflow Tower","text":"<p>This is a guide on how to use Nextflow Tower to monitor nf-core pipeline runs on DanGPU.</p> <p>All nf-core pipelines have been successfully configured for use on DanGPU.</p>"},{"location":"dangpu/nextflow_tower/#getting-started","title":"Getting started","text":"<p>If this is the first time you use Nextflow Tower, sign in and create a personal access token. You need to create a sample sheet before running any nf-core pipeline. Sample sheet format varies according to pipeline and examples can be found in the usage docs:</p> <ul> <li>nf-core/rnaseq usage docs</li> <li>nf-core/chipseq usage docs</li> <li>nf-core/cutandrun usage docs</li> </ul>"},{"location":"dangpu/nextflow_tower/#running-pipelines","title":"Running pipelines","text":"<p>Use the helper script nf-core_tower.sh to run DanGPU nf-core configs with Tower.</p> <pre><code>cd .local/bin\n\n# Start a new tmux session\ntmux new -s session_name\n\n# Export your personal tower access token:\nexport TOWER_ACCESS_TOKEN=your_access_token\n</code></pre> <p>Launch desired nf-core pipeline using helper script. Usage is</p> <pre><code>sh nf-core_tower.sh RUNNAME nextflow run &lt;OPTIONS&gt;\n</code></pre> <p>As a minimum, the pipeline name, samplesheet location, and genome must be defined, e.g. for rnaseq:</p> <pre><code>sh nf-core_tower.sh MYPAPER_2023 nextflow run nf-core/rnaseq -r 3.8.1 --input samplesheet.csv --genome mm10\n</code></pre>"},{"location":"dangpu/nextflow_tower/#tower-cli-installation","title":"Tower CLI installation","text":"<p>The tower cli1 is required to be installed only once to connect the DanGPU as a computing resource. Afterward, it's not required any more2.</p> <pre><code># Download the latest version of Tower CLI:\nwget https://github.com/seqeralabs/tower-cli/releases/download/v0.7.3/tw-0.7.3-linux-x86_64\n\n# Make the file executable and move to directory accessible by $PATH variable:\nmkdir ~/.local/bin &amp;&amp; mv tw-* tw &amp;&amp; chmod +x ~/.local/bin/tw\n</code></pre> <ol> <li> <p>Tower CLI configuration \u21a9</p> </li> <li> <p>Tower Agent \u21a9</p> </li> </ol>"},{"location":"dangpu/packages/","title":"Packages","text":""},{"location":"dangpu/packages/#podman","title":"Podman","text":""},{"location":"dangpu/packages/#setup","title":"Setup","text":"<p>Storage for Podman needs to be configured to fix UID errors when running on UTF filesystem:</p> <pre><code>mkdir -p ~/.config/containers\ncp /maps/projects/dan1/apps/podman/4.0.2/storage.conf $HOME/.config/containers/\n</code></pre> <p>Rootless Podman also requires username and allowed UID range to be listed in /etc/subuid and /etc/subgid</p> <p>List running containers and run a publically available container image to confirm Podman is working:</p> <pre><code>podman ps\npodman run -it docker.io/library/busybox\n</code></pre>"},{"location":"dangpu/packages/#running-the-ku-sund-dangpu-nf-core-config-with-podman","title":"Running the KU SUND DANGPU nf-core config with Podman","text":"<p>Currently this is not practical because file permissions cause the following error:</p> <pre><code>error during container init: error setting cgroup config for procHooks process: cannot set memory limit: container could not join or create cgroup\n</code></pre> <p>The nf-core config file, podman.config, can be found at /scratch/Brickman/pipelines/</p> <p>Specify podman.config in nextflow run options to run a pipeline with Podman, e.g. for the rnaseq test profile:</p> <pre><code>nextflow run nf-core/rnaseq -r 3.8.1 -c podman.config -profile test --outdir nfcore_test\n</code></pre>"}]}