{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>Welcome to the Brickman Lab wiki!</p> <p>Here you can find documentation for our analysis workflows.</p> <p>For more information about our research, visit the Brickman Group website.</p>"},{"location":"rdm-decisions/","title":"RDM decisions","text":""},{"location":"rdm-decisions/#naming-convesions","title":"Naming convesions","text":"<ul> <li>date format: <code>YYYYMMDD</code></li> </ul>"},{"location":"rdm-decisions/#synchronization","title":"Synchronization","text":"<p>We will have to setup a cron job to perform one-way sync between the <code>/projects</code> folder and <code>NGS_data</code> folder. All the analysis will be done on danGPU server, with no exceptions!</p> <p>After project is done and published, it will be moved to <code>NGS_data</code>.</p>"},{"location":"rdm-decisions/#template-engine","title":"Template engine","text":"<p>We are currently using cookiecutter to generate a folder structure. There is another alternative called cruft which allows us to validate and sync old templates with the latest. I think this could be a better alternative to cookiecutter.</p> <p>Note: I think cruft sounds very useful if we might change the project template in future. Is it as easy to use as cookiecutter when just routinely creating the project folder (should be easy to use for all members of the lab)?</p>"},{"location":"rdm-decisions/#data-management-catalogue","title":"Data management catalogue","text":"<p>@SLundregan has already build a prototype for NGS_data folder. We could do the same for the assays, this way we can make a quick decisions on which assays to to combine when publishing papers.</p> <p>Since assays will follow a folder structure, we could just traverse all the folders and combine <code>description.yaml</code> and <code>metadata.yaml</code>. Better solution would be to have a database, but that's an extra complexity on top.</p>"},{"location":"rdm-decisions/#data-management-structure","title":"Data management structure","text":""},{"location":"rdm-decisions/#assay","title":"Assay","text":"<p>There should be an <code>Assays</code> folder that will contain all experimental datasets (raw files and pipeline processed files). Inside <code>Assays</code> there will be subfolders named after a unique ID and the date it was created:</p> <pre><code>&lt;Assay-ID&gt;_YYYYMMDD\n</code></pre> <p>\\ should be easily identifiable. For example: <ul> <li>NGS assays like ChIP-seq, RNA-seq, ATAC-seq will be named: <code>NGSXX</code>, where:</li> <li>NGS will denote that the assay is a NGS experiment.</li> <li>XX is a two digit code identifier that will show what kind of NGS assay it is.  Similar to ICD codes.</li> <li>Proteomics assays will be named: <code>PROTXX</code>, where:</li> <li>PROT denotes the assay is Mass Spectometry Assay</li> <li>XX is a two digit code identifier that will show what kind of Mass Spectrometry assay it is.</li> <li>Multi-omics assays could be named: <code>OMICSXX</code>. This are assays that combines different types of OMICS, like CITEseq or RIME-ChIPseq.</li> <li>Imaging assays: ??</li> </ul> <p>Note: we do not know much about proteomics and mass spec data, neither imaging data.</p> <p>Examples:</p> <ul> <li>NGS01_20230101: NGS01 is the identifier for bulk RNAseq analysis. This assay was made on 1st January 2023.</li> <li>NGS10_20221202: NGS10 is the ID for ChIPseq.</li> </ul> <p>We could probably reserve ranges of ID for particular type of assays. For example, ATAC-seq and ChIP-seq could belong to the same range category <code>Chromatin accessibility/binding</code>, e.g. NGS10-NGS20, while RNA-seq techniques could be NGS00-NGS10.</p> <p>Note: Should the identifiers be more human readable to easily find the correct dataset, e.g. NGS_bulkRNA_20230101? Otherwise a key for the 2 digit identifiers should be stored in a common and easy to find place like Brickmanlab wiki.</p>"},{"location":"rdm-decisions/#folder-structure","title":"Folder Structure","text":"<pre><code>&lt;Assay-ID&gt;_20230424\n\u251c\u2500\u2500 description.yaml\n\u251c\u2500\u2500 metadata.yaml\n\u251c\u2500\u2500 pipeline.md\n\u251c\u2500\u2500 processed\n\u2514\u2500\u2500 raw\n   \u251c\u2500\u2500 fastq\n   \u2514\u2500\u2500 samplesheet.csv\n</code></pre> <ul> <li>description.yaml: longer description of the assay in yaml format.</li> <li>metadata.yaml: metadata file of the assay describing different keys, such as: assay_ID, owner, date, processing_date, processing_pipeline, processed_by, sample_number, organism, short_description, origin (external or internal).</li> <li>pipeline.md: description of the pipeline used to process raw data??</li> <li>processed: folder with results of the preprocessing pipeline. Contents depend on the pipeline used</li> <li>raw: folder with the raw data.</li> <li>fastq:In the case of NGS assays, there should be fastq files.</li> <li>samplesheet.csv: file that contains metadata information for the samples. This file is used to run the nf-core pipelines. Ideally, it will also contain a column with info regarding the experimental variables and batches so it can be used for down stream analysis as well.</li> </ul>"},{"location":"rdm-decisions/#project","title":"Project","text":"<p>There should be another folder called <code>Projects</code> that will contain project information and data analysis.</p> <p>A project may use one or more assays to answer a scientific question. This should be, for example, all the data analysis related to a publication.</p> <p>The project folder should be named after a unique identifier, such as:</p> <pre><code>&lt;Project-ID&gt;_YYYYMMDD\n</code></pre> <p>\\ might consist on the acronym of the owner of the project folder such as <code>JARH_20230101</code>. <p>Note: I think this system might not be ideal, any other suggestion?</p>"},{"location":"rdm-decisions/#folder-structure_1","title":"Folder structure","text":"<pre><code>&lt;Project-ID&gt;_20230424\n\u251c\u2500\u2500 data\n\u2502  \u251c\u2500\u2500 assays\n\u2502  \u251c\u2500\u2500 external\n\u2502  \u2502  \u251c\u2500\u2500 mouse_cell_cycle_genes.json\n\u2502  \u2502  \u2514\u2500\u2500 mouse_cell_cycle_genes.rds\n\u2502  \u2514\u2500\u2500 processed\n\u251c\u2500\u2500 documents\n\u251c\u2500\u2500 notebooks\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 reports\n\u2502  \u2514\u2500\u2500 figures\n\u251c\u2500\u2500 requirements.txt\n\u251c\u2500\u2500 results\n\u2514\u2500\u2500 scripts\n</code></pre> <ul> <li>data: folder that contains symlinks or shortcuts to where the data is, avoiding copying and modification of original files.</li> <li>documents: folder containing word documents, slides or pdfs related to the project, such as explanations on the data or project, papers, etc.</li> <li>notebooks: folder containing jupyter notebooks, R markdown or Quarto files with the actual data analysis. Using annotated notebooks is ideal for reproducibility and readability purposes.</li> <li>README.md:</li> <li>reports: notebooks can be knitted or transformed into html/docx/pdfs versions ideal for sharing with colleagues and also have a formal report of the data analysis prodecure.</li> <li>figures: each knitted document might produce figures. The figures will be saved under a subfolder named after the notebook that created them. This is for provenance purposes so you know which notebook created which images.</li> <li>results: results from the data analysis, such as tables with differentially expressed genes, enrichment results, etc. These results should be saved under a subfolder named after the notebook that created them. This is for provenance purposes so you know which notebook created which results.</li> <li>scripts: fodler containing helper scripts needed to run data analysis or reproduce the work of the folder</li> </ul> <p>Note: maybe we can make an environment folder or put inside the scripts folder a Dockerfile that should give you an environment where you can reproduce the results of the folder?</p> <p>Note2: shouldn't there be also a project_metadata.yaml here as well? also a project_description.yaml too? Note3: I guess this info would be contained in the README.md? The README/project_description could direct the reader to the the project_metatdata.yaml in the assays folder. Will there be a separate project_metadata.yaml for each dataset used (e.g. if both CHIPseq and RNAseq are used in a given project) or a single project_metadata.yaml with metatadata for all datasets used in the project?</p>"},{"location":"rdm/","title":"Research Data Management Guidelines","text":""},{"location":"rdm/#assay-code-names","title":"Assay code names","text":"<ul> <li><code>CHIP</code>: ChIP-seq</li> <li><code>RNA</code>: RNA-seq</li> <li><code>ATAC</code>: ATAC-seq</li> <li><code>SCR</code>: scRNA-seq</li> <li><code>PROT</code>: Mass Spectrometry Assay</li> <li><code>CAT</code>: Cut&amp;Tag</li> <li><code>CAR</code>: Cut&amp;Run</li> <li><code>RIME</code>: Rapid Immunoprecipitation Mass spectrometry of Endogenous proteins</li> </ul>"},{"location":"tools/","title":"Bioinformatics tools","text":"Tool Description NGS Language Link Functional enrichment on genomic regions CHIP-seq ATAC-seq R https://github.com/jokergoo/rGREAT Pseudotime inference scRNA-seq Python https://github.com/LouisFaure/scFates nan Single-cell analysis package scRNA-seq Python https://github.com/scverse/scanpy nan AI probabilistic package for transfer learning DR and more scRNA-seq Python https://github.com/scverse/scvi-tools Gene set enrichment analysis on steroids scRNA-seq Python https://github.com/zqfang/GSEApy nan UpsetR on stereoids (complicated Venn Diagrams) Plotting R https://github.com/krassowski/complex-upset nan Complex heatmap Plotting Python https://github.com/DingWB/PyComplexHeatmap nan"},{"location":"dangpu/","title":"DanGPU","text":"<p>For starting on the server make sure to read:</p> <ul> <li>DanGPU manual</li> <li>Genomics Platform wiki</li> <li>platforms: JupyterHub and RStudio</li> </ul>"},{"location":"dangpu/#first-time-on-server","title":"First time on server","text":""},{"location":"dangpu/#1-run-the-command-to-do-basic-setup-and-logout-afterwards","title":"1. Run the command to do basic setup and logout afterwards","text":"<pre><code>/maps/projects/dan1/apps/etc/init_dangpu_env.sh &amp;&amp; \\\necho \"export MODULEPATH=/maps/projects/dan1/apps/.modules:\\${MODULEPATH}\" &gt;&gt; $HOME/.bash_profile\n</code></pre>"},{"location":"dangpu/#2-create-extra-simlinks-for-folders","title":"2. Create extra simlinks for folders","text":"<pre><code>ln -s /maps/projects/dan1/people/$USER $HOME/datadir\nln -s ~/projects/data/Brickman $HOME/\n</code></pre>"},{"location":"dangpu/#3-setup-bash_profile","title":"3. Setup <code>.bash_profile</code>","text":"<p>To make your life simple, run the command below. This will give you access to extra helper scripts we have developed.</p> <pre><code>echo \"export PATH=/projects/dan1/data/Brickman/helper-scripts/helper-scripts:${PATH}\" &gt;&gt; ~/.bash_profile\n</code></pre>"},{"location":"dangpu/#folder-structure","title":"Folder structure","text":""},{"location":"dangpu/alphafold2/","title":"Alphafold 2","text":""},{"location":"dangpu/alphafold2/#1-running","title":"1. Running","text":""},{"location":"dangpu/alphafold2/#11-create-a-target-file","title":"1.1 Create a target file","text":"<pre><code># cat target.fasta\n&gt;query\nMAAHKGAEHHHKAAEHHEQAAKHHHAAAEHHEKGEHEQAAHHADTAYAHHKHAEEHAAQAAKHDAEHHAPKPH\n</code></pre>"},{"location":"dangpu/alphafold2/#12-setup-environments","title":"1.2. Setup environments","text":"<pre><code>srun -N 1 --ntasks-per-node=10 --gres=gpu:2 --pty bash\nmodule load miniconda/latest\nsource activate /maps/projects/dan1/data/Brickman/conda_envs/af2\ncd ~/projects/data/Brickman/alphafold\n</code></pre>"},{"location":"dangpu/alphafold2/#13-run-cli","title":"1.3. Run cli","text":"<pre><code>python run_alphafold.py \\\n--fasta_paths=~/projects/data/Brickman/target_01.fasta \\\n--output_dir=/scratch/tmp/alphatest \\\n--model_preset=monomer \\\n--db_preset=full_dbs \\\n--data_dir=~/projects/data/Alphafold2/24022023 \\\n--uniref30_database_path=~/projects/data/Alphafold2/24022023/uniref30/UniRef30_2021_03 \\\n--uniref90_database_path=~/projects/data/Alphafold2/24022023/uniref90/uniref90.fasta \\\n--mgnify_database_path=~/projects/data/Alphafold2/24022023/mgnify/mgy_clusters_2022_05.fa \\\n--pdb70_database_path=~/projects/data/Alphafold2/24022023/pdb70/pdb70 \\\n--template_mmcif_dir=~/projects/data/Alphafold2/24022023/pdb_mmcif/mmcif_files/ \\\n--obsolete_pdbs_path=~/projects/data/Alphafold2/24022023/pdb_mmcif/obsolete.dat \\\n--bfd_database_path=~/projects/data/Alphafold2/24022023/bfd/bfd_metaclust_clu_complete_id30_c90_final_seq.sorted_opt \\\n--max_template_date=2022-01-01 \\\n--use_gpu_relax\n</code></pre>"},{"location":"dangpu/alphafold2/#14-example-sbatch-script","title":"1.4. Example SBATCH script","text":"<pre><code>#!/bin/bash\n#SBATCH --job-name=AF2\n#SBATCH --gres=gpu:2\n#SBATCH --cpus-per-task=10\n#SBATCH --mail-type=BEGIN,END\n#SBATCH --mail-user=YOUR-EMAIL\nmodule load miniconda/latest\nsource activate af2\ncd ~/projects/data/Brickman/alphafold\nmkdir -p /scratch/tmp/alphatest\n\nsrun python run_alphafold.py \\\n--fasta_paths=~/projects/data/Brickman/target_01.fasta \\\n--output_dir=/scratch/tmp/alphatest \\\n--model_preset=monomer \\\n--db_preset=full_dbs \\\n--data_dir=~/projects/data/Alphafold2/24022023 \\\n--uniref30_database_path=~/projects/data/Alphafold2/24022023/uniref30/UniRef30_2021_03 \\\n--uniref90_database_path=~/projects/data/Alphafold2/24022023/uniref90/uniref90.fasta \\\n--mgnify_database_path=~/projects/data/Alphafold2/24022023/mgnify/mgy_clusters_2022_05.fa \\\n--pdb70_database_path=~/projects/data/Alphafold2/24022023/pdb70/pdb70 \\\n--template_mmcif_dir=~/projects/data/Alphafold2/24022023/pdb_mmcif/mmcif_files/ \\\n--obsolete_pdbs_path=~/projects/data/Alphafold2/24022023/pdb_mmcif/obsolete.dat \\\n--bfd_database_path=~/projects/data/Alphafold2/24022023/bfd/bfd_metaclust_clu_complete_id30_c90_final_seq.sorted_opt \\\n--max_template_date=2022-01-01 \\\n--use_gpu_relax\n</code></pre>"},{"location":"dangpu/alphafold2/#2-installation","title":"2. Installation","text":"<pre><code>conda create --prefix /maps/projects/dan1/data/Brickman/conda_envs/af2 python=3.8\nsource activate /maps/projects/dan1/data/Brickman/conda_envs/af2\n\nmamba install hmmer\npip install py3dmol\nmamba install pdbfixer==1.7\nmamba install -c conda-forge openmm=7.5.1\n\ncd /maps/projects/dan1/data/Brickman/\ngit clone --branch main https://github.com/deepmind/alphafold alphafold\npip install -r ./alphafold/requirements.txt\npip install --no-dependencies ./alphafold\n\n# stereo chemical props needs to be in common folder\nwget \u2013q \u2013P  /maps/projects/dan1/data/Brickman/alphafold/alphafold/common/ https://git.scicore.unibas.ch/schwede/openstructure/-/raw/7102c63615b64735c4941278d92b554ec94415f8/modules/mol/alg/src/stereo_chemical_props.txt\n\n# skipping content part\nmkdir -p ./alphafold/data/params &amp;&amp; cd ./alphafold/data/params\nwget https://storage.googleapis.com/alphafold/alphafold_params_colab_2022-12-06.tar\ntar --extract --verbose --preserve-permissions --file alphafold_params_colab_2022-12-06.tar\npip install ipykernel ipywidgets tqdm\npip install --upgrade scprep phate\n\n# Install jax\nmodule load miniconda/latest\nmodule load cuda/11.4 cudnn/8.2.2\nexport CUDA_VISIBLE_DEVICES='3'\npip install \"jax[cuda11_cudnn82]\" -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\n\n# fix last issues\nmamba install -c conda-forge -c bioconda hhsuite\nmamba install -c bioconda kalign3\npip install numpy==1.21.6\n</code></pre>"},{"location":"dangpu/alphafold2/#21-download-references","title":"2.1. Download references","text":"<p>Note</p> <p>Downloading references will not work on one try, had to do a lot of manual re-running of scripts.</p> <pre><code># create folder\nmkdir -p ~/projects/data/Alphafold2/24022023\ncd ~/projects/data/Alphafold2/24022023\n\n# Download all databases\nsh download_all_data.sh ~/projects/data/Alphafold2/24022023/ &gt; download.log 2&gt; download_all.log\n\n# Some fix-ups\n# mmCIF will not work because the firewall blocks the port, so I found this workaroud online\n# ref: https://github.com/deepmind/alphafold/issues/196\nwget -e robots=off -r --no-parent -nH --cut-dirs=7 -q ftp://ftp.ebi.ac.uk/pub/databases/pdb/data/structures/divided/mmCIF/ -P \"${RAW_DIR}\"\n# Last step is to fix all the permissions\nchmod -R 755 24022023/\n</code></pre>"},{"location":"dangpu/alphafold2/#references","title":"References","text":"<ul> <li>ifb-elixirfr</li> <li>deepmind/alphafold</li> </ul>"},{"location":"dangpu/conda/","title":"Conda","text":""},{"location":"dangpu/conda/#setup-on-dangpu","title":"Setup on DanGPU","text":"<pre><code>channels:\n- conda-forge\n- bioconda\n- defaults\nenvs_dirs:\n- /projects/dan1/people/$USER/envs/\n- ~/projects/data/Brickman/conda/envs\npkgs_dirs:\n- ~/.conda/pkgs/\n</code></pre>"},{"location":"dangpu/conda/#setup-on-ku-computer","title":"Setup on KU-computer","text":"<p>Go here and download Miniconda PKG not BASH. If you're running M1/2 please follow this guideline.</p>"},{"location":"dangpu/conda/#example-for-chip-seq-setup","title":"Example for CHIP-seq setup","text":"<pre><code>conda create --name chipseq python=3.6\nconda activate chipseq\nconda install -c bioconda deeptools bedtools\npip install intervene\n</code></pre>"},{"location":"dangpu/nextflow_tower/","title":"Nextflow Tower","text":"<p>This is a guide on how to use Nextflow Tower to monitor nf-core pipeline runs on DanGPU.</p> <p>All nf-core pipelines have been successfully configured for use on DanGPU.</p>"},{"location":"dangpu/nextflow_tower/#getting-started","title":"Getting started","text":"<p>If this is the first time you use Nextflow Tower, sign in and create a personal access token. You need to create a sample sheet before running any nf-core pipeline. Sample sheet format varies according to pipeline and examples can be found in the usage docs:</p> <ul> <li>nf-core/rnaseq usage docs</li> <li>nf-core/chipseq usage docs</li> <li>nf-core/cutandrun usage docs</li> </ul>"},{"location":"dangpu/nextflow_tower/#running-pipelines","title":"Running pipelines","text":"<p>Use the helper script nf-core_tower.sh to run DanGPU nf-core configs with Tower.</p> <pre><code>cd .local/bin\n\n# Start a new tmux session\ntmux new -s session_name\n\n# Export your personal tower access token:\nexport TOWER_ACCESS_TOKEN=your_access_token\n</code></pre> <p>Launch desired nf-core pipeline using helper script. Usage is</p> <pre><code>sh nf-core_tower.sh RUNNAME nextflow run &lt;OPTIONS&gt;\n</code></pre> <p>As a minimum, the pipeline name, samplesheet location, and genome must be defined, e.g. for rnaseq:</p> <pre><code>sh nf-core_tower.sh MYPAPER_2023 nextflow run nf-core/rnaseq -r 3.8.1 --input samplesheet.csv --genome mm10\n</code></pre>"},{"location":"dangpu/nextflow_tower/#tower-cli-installation","title":"Tower CLI installation","text":"<p>The tower cli1 is required to be installed only once to connect the DanGPU as a computing resource. Afterward, it's not required any more2.</p> <pre><code># Download the latest version of Tower CLI:\nwget https://github.com/seqeralabs/tower-cli/releases/download/v0.7.3/tw-0.7.3-linux-x86_64\n\n# Make the file executable and move to directory accessible by $PATH variable:\nmkdir ~/.local/bin &amp;&amp; mv tw-* tw &amp;&amp; chmod +x ~/.local/bin/tw\n</code></pre> <ol> <li> <p>Tower CLI configuration \u21a9</p> </li> <li> <p>Tower Agent \u21a9</p> </li> </ol>"},{"location":"dangpu/packages/","title":"Podman","text":""},{"location":"dangpu/packages/#setup","title":"Setup","text":"<p>Storage for Podman needs to be configured to fix UID errors when running on UTF filesystem:</p> <pre><code>mkdir -p ~/.config/containers\ncp /maps/projects/dan1/apps/podman/4.0.2/storage.conf $HOME/.config/containers/\n</code></pre> <p>Rootless Podman also requires username and allowed UID range to be listed in /etc/subuid and /etc/subgid</p> <p>List running containers and run a publically available container image to confirm Podman is working:</p> <pre><code>podman ps\npodman run -it docker.io/library/busybox\n</code></pre>"},{"location":"dangpu/packages/#running-the-ku-sund-dangpu-nf-core-config-with-podman","title":"Running the KU SUND DANGPU nf-core config with Podman","text":"<p>Currently this is not practical because file permissions cause the following error:</p> <pre><code>error during container init: error setting cgroup config for procHooks process: cannot set memory limit: container could not join or create cgroup\n</code></pre> <p>The nf-core config file, podman.config, can be found at /scratch/Brickman/pipelines/</p> <p>Specify podman.config in nextflow run options to run a pipeline with Podman, e.g. for the rnaseq test profile:</p> <pre><code>nextflow run nf-core/rnaseq -r 3.8.1 -c podman.config -profile test --outdir nfcore_test\n</code></pre>"}]}